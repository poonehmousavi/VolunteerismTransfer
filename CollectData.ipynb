{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "# from pandas import json_normalize\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select your output and input Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path=\"/home/VolunteerismTransfer/data/new/data/\"\n",
    "base_path=\"/home/VolunteerismTransfer/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and Process All Labeled TREC DATA 2018- 2019-2020A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_volunteer_tweets(postCategories):\n",
    "    # if 'Volunteer' in postCategories and 'Donations' in postCategories and 'GoodsServices' in postCategories:\n",
    "    #     return 'Vol-Don-Req'\n",
    "    # elif 'Volunteer' in postCategories and 'Donations' in postCategories:\n",
    "    #     return 'Vol-Don'\n",
    "    # elif 'Volunteer' in postCategories and 'GoodsServices' in postCategories:\n",
    "    #     return 'Vol-Req'\n",
    "    # elif 'Donations' in postCategories and 'GoodsServices' in postCategories:\n",
    "    #     return 'Don-Req'\n",
    "    if 'Volunteer' in postCategories or 'Donations' in postCategories or 'GoodsServices' in postCategories:\n",
    "        return 'Volunteer'\n",
    "    # elif 'Donations' in postCategories:\n",
    "    #     return 'Donations'\n",
    "    # elif 'GoodsServices' in postCategories:\n",
    "    #     return 'GoodServices'\n",
    "    else:\n",
    "        return \"Non_Volunteer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trunc_excessive_data_prop(data):\n",
    "    for index, row in data.iterrows():\n",
    "        if row['entities.hashtags'] != []:\n",
    "            data.loc[index, \"hashtags\"] = str([element['text'] for element in json.loads(row['entities.hashtags'])])\n",
    "        if row['entities.urls'] != []:\n",
    "            if(row.eventid == 'hurricaneFlorence2018' ):\n",
    "                data.loc[index, \"urls\"] = str(\n",
    "                    [element['expandedURL'] for element in json.loads(row['entities.urls'])])\n",
    "            else:\n",
    "                data.loc[index, \"urls\"] = str([element['expanded_url'] for element in json.loads(row['entities.urls'])])\n",
    "            if row['entities.user_mentions'] != []:\n",
    "                data.loc[index, \"user_mentions.id\"] = str([element['id'] for element in json.loads(row['entities.user_mentions'])])\n",
    "                data.loc[index, \"user_mentions.name\"] = str([element['name'] for element in json.loads(row['entities.user_mentions'])])\n",
    "\n",
    "\n",
    "\n",
    "    data.drop(\n",
    "            columns=['entities.hashtags', 'entities.urls','entities.user_mentions'],inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_label_unlable():\n",
    "\n",
    "    old_labels=pd.read_json(base_path+\"TRECIS_2018_2019-labels.json\", orient='records')\n",
    "    old_labels.rename(columns={\"postCategories\": \"categories\",\"postPriority\":\"priority\"}, inplace=True)\n",
    "    old_labels = old_labels[['postID', 'categories', 'priority']]\n",
    "    l=old_labels.groupby(['postID'])['categories'].agg('sum')\n",
    "    old_labels=old_labels.drop(columns=['categories'])\n",
    "    old_labels=pd.merge(l,all_trec, on='postID', how='inner')\n",
    "\n",
    "\n",
    "    new_label = pd.read_json(base_path+\"label_2020.json\", orient='records', lines=True)\n",
    "    new_label = new_label[['postID', 'categories', 'priority']]\n",
    "    l=new_label.groupby(['postID'])['categories'].agg('sum')\n",
    "    new_label=new_label.drop(columns=['categories'])\n",
    "    new_label=pd.merge(l,all_trec, on='postID', how='inner')\n",
    "\n",
    "    labels= old_labels.append(new_label)\n",
    "\n",
    "    unlabel = pd.read_json(base_path+\"out/all-2018-2019-2020.json\", orient='records', lines=True)\n",
    "    unlabel.rename(columns={\"id\": \"postID\"}, inplace=True)\n",
    "    labels.drop_duplicates(['postID'], keep='first', inplace=True)\n",
    "    labels['categories']=labels.groupby(['postID'])['categories'].agg('sum').values\n",
    "    labels['categories']=labels.groupby(['postID'])['categories'].agg('sum').values\n",
    "    unlabel.drop_duplicates(['postID'], keep='first', inplace=True)\n",
    "\n",
    "    result = pd.merge(labels,unlabel, on='postID', how='inner')\n",
    "    result['volunteerLabels'] = [label_volunteer_tweets(x) for x in result['categories']]\n",
    "    # volunteer_df= result.query('volunteerLabels != \"Non_Volunteer\"')\n",
    "    volunteer_df=trunc_excessive_data_prop(result)\n",
    "    volunteer_df.to_json(output_path+\"labeled_TREC-2018-2019-2020A.json\", orient='records', lines=True)\n",
    "    return  volunteer_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_data=merge_label_unlable()\n",
    "trec_data['src']='trec'\n",
    "trec_data.drop(columns=['user_mentions.id','user.id','user.name','urls','hashtags','priority','categories'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_data= pd.read_json(output_path+\"labeled_TREC-2018-2019-2020A.json\", orient='records', lines=True)\n",
    "trec_data['src']='trec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and Process  CrisisNLP  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_CrissNLP_volunteer_tweets_2(label):\n",
    "    if 'Shelter and supplies' in label:\n",
    "        return 'Volunteer'\n",
    "    if 'Money' in label:\n",
    "            return 'Volunteer'\n",
    "    if 'Volunteer or professional services' in label:\n",
    "            return 'Volunteer'\n",
    "    if 'Humanitarian Aid Provided' in label:\n",
    "            return 'Volunteer'\n",
    "    if 'Needs of those affected' in label:\n",
    "            return 'Volunteer'\n",
    "    if 'Donations of money' in label:\n",
    "            return 'Volunteer'\n",
    "    if 'Donations of supplies and/or volunteer work' in label:\n",
    "            return 'Volunteer'\n",
    "    if 'Donation needs or offers or volunteering services' in label:\n",
    "            return 'Volunteer'\n",
    "    else:\n",
    "        return \"Non_Volunteer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crisisNLP_Evnet(filename):\n",
    "    filename=filename.lower()\n",
    "    if filename.startswith(\"2014_California_Earthquake\".lower()):\n",
    "        return \"2014_California_Earthquake\"\n",
    "    elif filename.startswith(\"2014_chile_earthquake\".lower()) :\n",
    "        return \"2014_chile_earthquake\"\n",
    "    elif filename.startswith(\"2014_ebola\"):\n",
    "        return \"2014_ebola\"\n",
    "    elif filename.startswith(\"2014_Hurricane_Odile_Mexico\".lower()):\n",
    "        return \"2014_Hurricane_Odile_Mexico\"\n",
    "    elif filename.startswith(\"2014_Iceland_Volcano\".lower()):\n",
    "        return \"2014_Iceland_Volcano\"\n",
    "    elif filename.startswith(\"2014_India_floods\".lower()):\n",
    "        return \"2014_India_floods\"\n",
    "    elif filename.startswith(\"2014_MERS\".lower()):\n",
    "        return \"2014_MERS\"\n",
    "    elif filename.startswith(\"2014_Middle_East_Respiratory_Syndrome\".lower()):\n",
    "        return \"2014_Middle_East_Respiratory_Syndrome\"\n",
    "    elif filename.startswith(\"2014_Pakistan_floods\".lower()):\n",
    "        return \"2014_Pakistan_floods\"\n",
    "    elif filename.startswith(\"2014_Philippines_Typhoon_Hagupi\".lower()):\n",
    "        return \"2014_Philippines_Typhoon_Hagupi\"\n",
    "    elif filename.startswith(\"2014_Philippines_Typhoon_Hagupi\".lower()) or filename.startswith(\"2014_Typhoon_Hagupit_en\".lower()):\n",
    "        return \"2014_Philippines_Typhoon_Hagupi\"\n",
    "    elif filename.startswith(\"2015_Cyclone_Pam\".lower()):\n",
    "        return \"2015_Cyclone_Pam\"\n",
    "    elif filename.startswith(\"2015_Nepal_Earthquake\".lower()):\n",
    "        return \"2015_Nepal_Earthquake\"\n",
    "    elif filename.startswith(\"Landslides_Worldwide\".lower()):\n",
    "        return \"Landslides_Worldwide\"\n",
    "    else:\n",
    "        return \"NA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_CrisisNLP_Data(file_path):\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "\n",
    "    for filename in os.listdir(file_path):\n",
    "        try:\n",
    "            if filename.endswith('tsv'):\n",
    "                df = pd.read_csv(file_path + filename,sep='\\t')\n",
    "            else:\n",
    "                df = pd.read_csv(file_path + filename)\n",
    "            df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "            eventid=get_crisisNLP_Evnet(filename)\n",
    "            df['volunteerLabels'] = [label_CrissNLP_volunteer_tweets_2(x) for x in df['label']]\n",
    "            df.rename(columns={\"tweet_id\": \"postID\", \"tweet_text\": \"text\"}, inplace=True)\n",
    "            df = df[['postID','text', 'volunteerLabels']]\n",
    "\n",
    "            for index, row in df.iterrows():\n",
    "                temp=row\n",
    "                temp['user_mentions.name'] =str(regex.findall(row.text))\n",
    "                temp['eventid'] = eventid\n",
    "                data = data.append(temp)\n",
    "\n",
    "        except:\n",
    "            print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "            print(filename)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crissi_nlp_data=get_all_CrisisNLP_Data(base_path+\"CrisisNLP_data/\")\n",
    "crissi_nlp_data['src']='crisis_nlp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crisisNLP_data=pd.read_json(output_path+\"all_crisisNLP.json\",orient='records', lines=True)\n",
    "crisisNLP_data['src']='crisis_nlp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crisisNLP_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge TREC IS  and CrisisNLP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_eventtype(eventid):\n",
    "    if 'earthquake' in eventid.lower():\n",
    "        return 'earthquake'\n",
    "    elif 'hurricane' in eventid.lower() or 'typhoon' in eventid.lower() or 'cyclone' in eventid.lower() or 'tornado' in eventid.lower():\n",
    "        return 'hurricane/typhoon/cyclone/tornado'\n",
    "    elif 'flood' in eventid.lower():\n",
    "        return 'flood'\n",
    "    elif 'wildfire' in eventid.lower() or 'bushfire' in eventid.lower() or 'fire' in eventid.lower():\n",
    "        return 'wildfire/bushfire'\n",
    "    elif 'bombing' in eventid.lower():\n",
    "        return 'bombing'\n",
    "    elif 'shooting' in eventid.lower():\n",
    "        return 'shooting'\n",
    "    elif 'explosion' in eventid.lower():\n",
    "        return 'explosion'\n",
    "    elif 'outbreak' in eventid.lower():\n",
    "        return 'outbreak'\n",
    "    elif 'collapse' in eventid.lower():\n",
    "        return 'collapse'\n",
    "    else:\n",
    "        return 'general'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data=crisisNLP_data.append(trec_data)\n",
    "labeled_data['event_type'] = [label_eventtype(x) for x in labeled_data['eventid']]\n",
    "labeled_data=labeled_data[[\"postID\",\"eventid\",\"event_type\",\"text\",\"volunteerLabels\",\"src\"]]\n",
    "labeled_data.to_json(output_path+\"Labeled.json\",orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data['l'] = [convert_label(x) for x in labeled_data['volunteerLabels']]\n",
    "labeled_data.query('l == 1').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get and Process NGO (Top Accounts) Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_account_data = pd.read_json(output_path + 'top-account_data.json', orient='records', lines=True)\n",
    "top_account_data['eventid'] = 'general'\n",
    "top_account_data['volunteerLabels'] = 'Volunteer'\n",
    "top_account_data['src'] = 'top-accounts'\n",
    "top_account_data['event_type'] = 'general'\n",
    "# data.drop(columns=['user_mentions.name'],inplace=True)\n",
    "# data = data.append(top_account_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labaled_with_NGO_Data=top_account_data.append(labeled_data)\n",
    "labaled_with_NGO_Data.to_json(output_path+\"NGO+Labeled Data.json\",orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get and Process Random TREC IS  Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unlabled_Data(file_path):\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "\n",
    "    for filename in os.listdir(file_path):\n",
    "\n",
    "            df = pd.read_json(file_path + filename, orient='records', lines=True)\n",
    "            df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "            eventid=filename.replace(\".json\", \"\")\n",
    "            df.rename(columns={\"id\": \"postID\", \"full_text\": \"text\"}, inplace=True)\n",
    "            df = df[['postID','text']]\n",
    "            df['eventid']=eventid\n",
    "            data = data.append(df)\n",
    "\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_data=get_unlabled_Data(base_path+\"random_unlabeled_sample/\")\n",
    "random_data['volunteerLabels'] = 'unkonwn'\n",
    "random_data['event_type'] = [label_eventtype(x) for x in random_data['eventid']]\n",
    "random_data['src']='random'\n",
    "random_data.to_json(output_path+\"random_data.json\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labaled_with_Random_Data=random_data.append(labeled_data)\n",
    "labaled_with_Random_Data.to_json(output_path+\"Random+Labeled Data.json\",orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge RAndom , NGO and Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labaled_with_Random_NGO_Data=labaled_with_Random_Data.append(top_account_data)\n",
    "labaled_with_Random_NGO_Data.to_json(output_path+\"NGO+Random+Labeled Data.json\",orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_account_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labaled_with_Random_NGO_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}