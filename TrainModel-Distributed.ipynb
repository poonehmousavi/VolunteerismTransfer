{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, cohen_kappa_score, precision_score, recall_score, \\\n",
    "    precision_recall_curve\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.semi_supervised import LabelSpreading, LabelPropagation\n",
    "from tqdm import tqdm\n",
    "import multiprocessing \n",
    "from time import sleep\n",
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.downloader import base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting multiprocessing\n",
      "  Using cached https://files.pythonhosted.org/packages/b8/8a/38187040f36cec8f98968502992dca9b00cc5e88553e01884ba29cbe6aac/multiprocessing-2.6.2.1.tar.gz\n",
      "    Complete output from command python setup.py egg_info:\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/tmp/pip-install-q7s30hnr/multiprocessing/setup.py\", line 94\n",
      "        print 'Macros:'\n",
      "                      ^\n",
      "    SyntaxError: Missing parentheses in call to 'print'. Did you mean print('Macros:')?\n",
      "    \n",
      "    ----------------------------------------\n",
      "\u001b[31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-install-q7s30hnr/multiprocessing/\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.3b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select your output and input Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path=\"/home/alip/PycharmProjects/VolunteerismTransfer/data/new/result/weak-supervison/new/\"\n",
    "base_path=\"/home/alip/PycharmProjects/VolunteerismTransfer/data/new/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "We have 5 Different Inputs:\n",
    "    Labeled data ,\n",
    "    Labeled data + NGO accounts(labeled as  \"volunteer\"),\n",
    "    Labeled data + NGO accounts(labeled by label prop ),\n",
    "    Labeled data + random Data(labeled by label prop),\n",
    "    Labeled data + NGO (labeled by label prop) + random Data(labeled by label prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(base_path+'data/new/PR_NGO+Labeled Data.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get hierarchical Event Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_weight(event_type, target):\n",
    "\n",
    "    if event_type == target:\n",
    "        return 10\n",
    "    elif get_sample_category(event_type) == get_sample_category(target):\n",
    "        return 6\n",
    "    elif get_sample_category(event_type)=='general':\n",
    "        return 1\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_category(event_type):\n",
    "    if 'earthquake' in event_type or \"hurricane/typhoon/cyclone/tornado\" in event_type or \"flood\" in event_type or \"wildfire/bushfire\" in event_type or \"outbreak\" in event_type:\n",
    "        return 'natural'\n",
    "    elif \"bombing\" in event_type or \"shooting\" in event_type or \"explosion\" in event_type or \"collapse\" in event_type:\n",
    "        return \"manmade\"\n",
    "    else:\n",
    "        return \"general\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_sgd = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf',  TfidfTransformer()),\n",
    "    ('nb', SGDClassifier(loss='hinge')),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SVM Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_different_sampling_strategy(data,seed,heldout_event):\n",
    "    performance_result={}  \n",
    "    sampling_strategies=['none','up','down','up-with-same-eventtype','up-with-same-eventCategory']\n",
    "    for sampling_strategy in tqdm(sampling_strategies):\n",
    "        if sampling_strategy== 'none' or sampling_strategy=='up' or sampling_strategy=='down':\n",
    "            performance_result[sampling_strategy+'-without-upweight']=evaluate_model(data,heldout_event,sampling_strategy,up_weighting=False,seed_number=seed)\n",
    "            performance_result[sampling_strategy+'-with-upweight']=evaluate_model(data,heldout_event,sampling_strategy,up_weighting=True,seed_number=seed)\n",
    "        else:\n",
    "            performance_result[sampling_strategy]=evaluate_model(data,heldout_event,sampling_strategy,up_weighting=False,seed_number=seed)\n",
    "    return performance_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(data,heldout_event, sampling_strategy,up_weighting,seed_number,label_spreading=False):\n",
    "    #Decide with field to use for training as a gold-label\n",
    "    if label_spreading== True:\n",
    "        lab='label_spread'\n",
    "    else:\n",
    "        lab='label'\n",
    "    \n",
    "    # Split Test and trainig data based on heldout_event/heldout_eventType  and get the resample training data     \n",
    "    main_training=data[data[groupby_col] != heldout_event]\n",
    "    test = data[data[groupby_col] == heldout_event]  \n",
    "    training=get_sample_data(main_training,seed_number,label_spreading)\n",
    "        \n",
    "    #Randomely give weights to each sample,then set higher weights to event of same-type\n",
    "    if up_weighting == True:\n",
    "        training['sample_weight'] = 100 * np.abs(np.random.randn(training.shape[0]))\n",
    "        training['sample_weight'] = np.where(training['event_type'] ==  test.iloc[0].event_type,\n",
    "                                training['sample_weight'] * 10,\n",
    "                                training['sample_weight'])\n",
    "    #Split training data to volunteer and non-volunterrs for further use in applying sampling strategies\n",
    "    vol = training.loc[training[lab] == 1]\n",
    "    non_vol = training.loc[training[lab] == 0]\n",
    "    \n",
    "    #Upsampling Strategy\n",
    "    if sampling_strategy == 'up':\n",
    "        training = non_vol.append(vol.sample(n=len(non_vol), replace=True), ignore_index=True)\n",
    "\n",
    "    #Downsampling Strategy\n",
    "    elif sampling_strategy == 'down':\n",
    "        training = vol.append(non_vol.sample(n=len(vol), replace=False), ignore_index=True)\n",
    "\n",
    "    #when up-sampling, resample primarily from events of the same type\n",
    "    elif sampling_strategy== \"up-with-same-eventtype\":\n",
    "        training['sample_weight'] = 1\n",
    "        training['sample_weight'] = np.where(training['event_type'] == test.iloc[0].event_type,\n",
    "                                                 training['sample_weight'] * 10,\n",
    "                                                 training['sample_weight'])\n",
    "        vol = training.loc[training[lab] == 1]\n",
    "        non_vol = training.loc[training[lab] == 0]\n",
    "        training = non_vol.append(vol.sample(n=len(non_vol),weights='sample_weight', replace=True), ignore_index=True)\n",
    "    #when up-sampling, resample primarily from events of the same “kind” of event (manmade vs. natural) \n",
    "    elif sampling_strategy== \"up-with-same-eventCategory\":\n",
    "        training['sample_weight'] = [get_sample_weight(x,test.iloc[0].event_type) for x in training['event_type']]\n",
    "        vol = training.loc[training[lab] == 1]\n",
    "        non_vol = training.loc[training[lab] == 0]\n",
    "        training = non_vol.append(vol.sample(n=len(non_vol),weights='sample_weight', replace=True), ignore_index=True)\n",
    "\n",
    "\n",
    "    X_train =[str(x) for x in training['processed_text']]\n",
    "    X_test = [str(x) for x in test['processed_text']]\n",
    "    y_train = training[lab]\n",
    "    y_test = test['label']\n",
    "    if up_weighting== True:\n",
    "        model = pipeline_sgd.fit(X_train, y_train,nb__sample_weight=training['sample_weight'])\n",
    "    else:\n",
    "        model = pipeline_sgd.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    y_predict = model.predict(X_test)\n",
    "    recall =  recall_score(y_test, y_predict,zero_division=0)\n",
    "    precision = precision_score(y_test, y_predict,zero_division=0)\n",
    "    from sklearn.metrics import f1_score\n",
    "    f1_score=f1_score(y_test, y_predict,zero_division=0)\n",
    "    return {groupby_col: test.iloc[0].eventid+'-'+str(seed_number),'src': test.iloc[0].src, 'precision': precision, 'recall': recall, 'f1_score': f1_score}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_data(data, seed_number,label_spreading=False):\n",
    "        \n",
    "    labeled_data = data.query(\"src == 'trec' or src == 'crisis_nlp'\")\n",
    "    other = data.query(\"src != 'trec' and src != 'crisis_nlp'\")\n",
    "    N=len(labeled_data)\n",
    "    sample_data=labeled_data.sample(n=N, random_state=seed_number, replace=True)\n",
    "    sample_data=sample_data.append(other)\n",
    "    if label_spreading==True:\n",
    "        sample_data['ft_features'] = [vectorize(str(s)) for s in sample_data[\"processed_text\"]]\n",
    "        sample_data['l']=sample_data.apply(lambda row: row['ft_features'].size, axis=1)\n",
    "        sample_data=sample_data.query(\"l >1\")\n",
    "        sample_data.drop( columns=['l'],inplace=True)\n",
    "        X_data = sample_data['ft_features']\n",
    "        y_labels = sample_data['label'].copy()\n",
    "        y_labels[sample_data['src'].isin(['top-accounts', 'random'])] = -1\n",
    "        label_prop_model = LabelSpreading(kernel='knn', n_jobs=-1)\n",
    "        y_learned = label_prop_model.fit(X_data.tolist(), y_labels).transduction_\n",
    "        sample_data[\"label_spread\"] = y_learned\n",
    "        sample_data=sample_data.query(\"src == 'trec' or src=='crisis_nlp' or ((src=='top-accounts' or src=='random') and label_spread==1 )\")\n",
    "    return sample_data \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "print(api.load('glove-twitter-200', return_path=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, cohen_kappa_score, precision_score, recall_score, \\\n",
    "    precision_recall_curve\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.semi_supervised import LabelSpreading, LabelPropagation\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.downloader import base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(base_dir, 'glove-twitter-100', 'glove-twitter-100.gz')\n",
    "model_gensim = KeyedVectors.load_word2vec_format(path)\n",
    "\n",
    "wvs = model_gensim.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    use_idf=True,\n",
    "    smooth_idf=False,\n",
    "    norm=None,  # Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    max_features=10000,\n",
    "    min_df=4,\n",
    "    max_df=0.501\n",
    ")\n",
    "\n",
    "analyzer = vectorizer.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(sentence):\n",
    "\n",
    "    tokenized = [t for t in analyzer(sentence)]\n",
    "\n",
    "    wv_vecs = []\n",
    "    for t in tokenized:\n",
    "\n",
    "        try:\n",
    "            v = wvs[t]\n",
    "            norm = np.linalg.norm(v)\n",
    "            normed_v = (v / norm)\n",
    "            wv_vecs.append(normed_v)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    m = np.array(wv_vecs)\n",
    "    normed_m = np.mean(m, axis=0)\n",
    "\n",
    "    normed_m = np.nan_to_num(normed_m)\n",
    "\n",
    "\n",
    "    return normed_m\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = multiprocessing.Manager()\n",
    "\n",
    "# Define a list (queue) for tasks and computation results\n",
    "tasks = manager.Queue()\n",
    "results = manager.Queue()\n",
    "num_processes = 13\n",
    "pool = multiprocessing.Pool(processes=num_processes)\n",
    "processes = []\n",
    "for event in events:\n",
    "\n",
    "    if test.iloc[0].src=='trec' and test.shape[0] > 30:\n",
    "        # Set process name\n",
    "        process_name = event\n",
    "\n",
    "        # Create the process, and connect it to the worker function\n",
    "        for seed in seeds:\n",
    "            new_process = multiprocessing.Process(target=get_different_sampling_strategy, args=((data,seed_number=seed,heldout_event=event))\n",
    "\n",
    "        # Add new process to the list of processes\n",
    "        processes.append(new_process)\n",
    "\n",
    "        # Start the process\n",
    "        new_process.start('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:00<00:00, 80.78it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Specify how to split training and test data by chooisng value from ['eventid','event_type']\n",
    "#Only consider events with more than/equal to 30 volunteer tweets as a held-out event\n",
    "groupby_col='eventid'\n",
    "seeds=np.arange(1,21)\n",
    "all_events=data.groupby([groupby_col]).groups.keys()\n",
    "events=[]\n",
    "for event in tqdm(all_events):\n",
    "    training=data[data['eventid'] == event]\n",
    "    if training.iloc[0].src != 'trec':\n",
    "        continue;\n",
    "    vol=training.query(\"label == 1 \")\n",
    "    if vol.shape[0]>=30:\n",
    "               events.append(event)\n",
    "               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return dictiony contains result for applying different sampling strategies on sample of training data with initail seed \n",
    "#Key of the dictionary is the sampling strategy\n",
    "#this should be called  number of heldout_event * number of seeds\n",
    "result=get_different_sampling_strategy(data,seed_number=1,heldout_event='philippinesEarthquake2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return dictiony contains result for sample of training data with initail seed and specified sampling strategy\n",
    "#Label Spreading is applied if  label_spreading is set to True otherwise it is false by default\n",
    "result=evaluate_model(data,heldout_event='philippinesEarthquake2019',sampling_strategy='up',up_weighting=True,seed_number=1,label_spreading=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
